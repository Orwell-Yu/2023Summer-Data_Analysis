<font face = "华文中宋">

# 简单的数据挖掘与处理脚本使用介绍
<center> 2023 Summer </center>

-------

## 数据预处理脚本
### 生成独热编码 
本次项目中，打标后的表格满足以下条件：其中每一列都是一个下拉表单，而每行数据都是每一列表单的一个选项。所以我们考虑使用**独热编码（One-Hot Encoding）** 来将这些分类数据数值化。
独热编码是一种常用的特征编码方法，它将每个分类变量转换为一个二进制向量，其中只有一个元素为1，表示当前的选项，而其他元素都为0。这种编码方式可以将分类变量转换为数值变量，以便在机器学习模型中使用。
为了将`Excel`数据转化成独热编码，我们编写了一个简单的脚本`generate_onehot.py`。

其中，更换文件可以修改：
```Python
# 读取Excel文件  
data = pd.read_excel('matrix_ex02.xlsx')  
```
对于`matrix_ex02.xlsx`，我们只希望它第八行之后的数据转化为独热编码，所以我们选择了`[7:]`，更换文件之后只要改变这个数字即可。
```python
# 对每一列进行独热编码  
encoded_data = pd.get_dummies(data, columns=data.columns[7:])  
```


### 日期更新
为了便于使用`Excel`表格自带的数据透视分析，我们编写了一个简单的`date_update.py`脚本，用于读入一张表格，将具体日期格式转化为`"xxx年xxx月"`的形式。
其中，更换文件可以修改：
```Python
filepth='2023素材标签分析.xlsx'
```

### 数据分析、清洗与离群值筛选
**离群值（Outliers）** 是指在数据集中与其他观测值显著不同的异常值。它们可能是由于测量错误、数据录入错误、异常事件或统计误差等原因引起的。离群值的存在可能会对数据分析和建模产生负面影响，因此在进行数据分析前，通常需要对离群值进行筛选和清洗。
筛选离群值的方法可以有多种，常见的方法包括基于统计指标的方法和基于距离的方法：
1. 基于统计指标的方法：这种方法使用统计学原理来识别离群值。常见的统计指标包括均值、标准差、四分位数等。通过计算数据点与这些统计指标之间的偏差或距离，可以确定是否为离群值。例如，常用的方法是使用均值加减几倍标准差的范围来定义离群值的阈值。
2. 基于距离的方法：这种方法基于数据点之间的距离或相似度来判断离群值。常见的方法包括K近邻算法、聚类分析等。通过计算数据点与其他点之间的距离或相似度，可以识别出与其他点差异较大的离群值。

**数据清洗**的目的是识别和处理离群值，以提高数据的质量和准确性。数据清洗的原理和作用如下：
1. 原理：数据清洗通过识别和处理离群值来修复或剔除异常数据，以确保数据的可靠性和一致性。清洗过程通常包括以下步骤：
  - 离群值检测：使用适当的方法和阈值来识别离群值。
  - 离群值处理：根据具体情况，可以采取不同的处理方式，如替换为缺失值、平均值或中位数，或者直接剔除离群值所在的数据点。
  - 数据修复：对于存在错误或缺失值的数据，可以尝试使用插值、回归等方法进行修复，以恢复数据的完整性和准确性。
2. 作用：数据清洗对于数据分析和建模具有重要的作用：
  - 提高数据质量：清洗离群值可以减少异常数据对整体数据质量的影响，提高数据的准确性和可靠性。
  - 改善模型效果：离群值可能对数据模型产生不良影响，通过清洗离群值可以改善模型的拟合效果和预测准确性。
  - 保护分析结果的可信度：在进行统计分析和数据挖掘时，清洗离群值可以提高结果的可信度，避免误导性的分析结论。
  - 数据预处理：数据清洗是数据预处理的一部分，为后续的数据分析、建模和可视化提供干净、一致的数据集。
综上所述，离群值是与其他观测值显著不同的异常值，数据清洗通过识别和处理离群值来提高数据质量、改善模型效果和保护分析结果的可信度。清洗过程包括离群值检测和处理，可以基于统计指标或距离等方法进行。清洗后的数据可以更好地支持数据分析、建模和可视化等任务。

在这里，我们给出了一个脚本`Outlier.py`，它全部由函数组成，内部有已经实现的离群值筛选函数、图表生成函数以及尚未实现的数据清洗函数的接口，后续可以在其他脚本中以`import`的形式调用。

由于样本量不大，我们这次不考虑数据清洗。但是随着样本量上升，后续工作中完全可以添加关于数据清洗的函数。并随需求选择自己的清洗方式。

## 数据挖掘脚本

### 线性回归分析

**线性回归（Linear Regression）** 是一种用于建立线性关系的回归算法，用于预测一个连续的数值输出。它通过拟合一条直线（在一维情况下）或一个超平面（在多维情况下）来描述自变量与因变量之间的线性关系。线性回归的目标是找到最佳拟合线（或超平面），使得预测值与实际值之间的误差最小。

通过`LinearRegression.py`，我们对转化为独热编码的`csv`文件进行线性回归分析，并根据脚本生成线性回归的图片，打印斜率最大的自变量名称。

其中，更换输出文件夹可以修改：
```Python
# 创建存储图表的文件夹  
output_folder = 'LinearRegression_Analysis'  
if not os.path.exists(output_folder):  
    os.makedirs(output_folder)  
```
更换输出文件可以修改：
```Python
# 读取CSV文件  
data = pd.read_csv('one_hot.csv') 
```
对于`one_hot.csv`，我们只希望它第7列及之后的列作为自变量，第3~6列作为因变量。所以我们选择了`[6:]`和`[2:6]`，更换文件之后只要改变这些数字即可。
```python
# 获取输入变量和因变量  
X_columns = data.columns[6:]  # 选择第7列及之后的列作为输入变量  
y_columns = data.columns[2:6]  # 选择第3~6列作为因变
```

### XGBoost分析
**XGBoost（eXtreme Gradient Boosting）** 是一种基于梯度提升树的机器学习算法，可用于回归和分类问题。`XGBoost`回归是指使用`XGBoost`算法进行回归任务，即通过构建多个弱学习器的集合来拟合数据，并预测连续型目标变量的值。
而对于我们正在进行的小数据量的数据挖掘任务，选择`XGBoost`回归有以下好处：
1. 高准确性：`XGBoost`在处理小数据集时表现出色，它通过集成多个弱学习器来提高预测的准确性。`XGBoost`使用了一种特殊的优化技术，能够减少过拟合的风险，同时在训练集和测试集上都能获得较高的预测准确性。
2. 快速训练速度：`XGBoost`在处理小数据集时通常具有较快的训练速度。它通过并行计算和有效的数据结构优化，能够高效地处理大量特征和样本。这对于小数据集来说，可以更快地完成模型训练和调参的过程。
3. 对缺失值和异常值的鲁棒性：`XGBoost`能够处理缺失值和异常值，不需要对其进行额外的处理。它可以自动处理缺失值，并且在构建树的过程中对异常值具有一定的鲁棒性，不容易受到异常值的影响。
4. 特征重要性评估：`XGBoost`提供了对特征重要性的评估，可以帮助你了解哪些特征对于模型的预测性能最重要。这对于小数据集的特征选择和特征工程非常有帮助，可以帮助你更好地理解数据并选择最相关的特征。
总之，`XGBoost`回归在小数据量数据挖掘任务中具有高准确性、快速训练速度、对缺失值和异常值的鲁棒性以及特征重要性评估等优势。这些特点使得`XGBoost`成为处理小数据集的强大工具，并广泛应用于各种预测和回归问题。

而在此次任务中，由于数据中的`消耗`、`点击率`、`播放率`等变量受时间变化影响。我们考虑根据时间对它们进行降权处理。

在广告发布的数据挖掘实验中，考虑到`消耗`作为一个绝对值变量的特性以及时间的影响，我们选择使用**线性降权法**，其背后的理念是将上线日期较早的广告的权重逐渐降低，以减少其对整体结果的过度影响；而对于`点击率`、`3秒播放率`和`25%播放率`，根据广告逻辑，随着投放时间的增长，广告的曝光量逐渐饱和，导致点击率逐渐降低。由于点击率和播放率是以比例形式呈现的指标，而非累计值，因此使用**指数降权**方法可以更好地反映这种趋势，并提供更准确的分析结果。

通过`XGBoost.py`，我们实现了训练`XGBoost`模型并输出预测的视频配置以及打分表格。

其中，更换输出文件夹可以修改：
```Python
# 创建保存结果的目录
result_dir = "XGBoost_Analysis"
if os.path.exists(result_dir):
    # 如果目录已经存在，删除目录及其中的文件
    for file_name in os.listdir(result_dir):
        file_path = os.path.join(result_dir, file_name)
        os.remove(file_path)
    os.rmdir(result_dir)
os.mkdir(result_dir)
```
这里可以修改读入的`csv`文件:
```python
# 读取CSV文件
data = pd.read_csv("one_hot1.csv")
```
对于`one_hot1.csv`，我们的第2列是消耗，使用线性降权；第3列是前次展现费用，不需要降权；第4 ~ 6列是点击率和播放率，使用指数降权；第七列是时间，用于降权；第8列及以后是自变量。所以，我们对2，3列分别考虑，对4 ~ 6列使用循环。更换文件之后需要更改这三处的相应列，这里以“消耗”为例，更改目标变量和输入变量的位置，需要修改：
```Python
# 提取因变量列和自变量列,关于“消耗”对时间进行降权处理
target_column = data.columns[1]
feature_columns = data.columns[7:]
```
修改降权方式，需要修改：
```Python
# 线性降权
min_date = train_dates.min()
train_weights = (today - train_dates).dt.days.apply(lambda x: max(0,1 - x / (today - min_date).days))
```
值得一提的是，由于模型过拟合限制，我们输出最优解的备选集合只有输入的数据集，而没有采用遍历所有可能解的情况。当然，我们在脚本中预留了函数接口`XGBoost_testdata.py`，可以实现并输出遍历全部可行解的测试数据集。当然，需要注意的是：
  - 生成测试集时多个自变量本身具有限制关系
  - 可以先输出打分表格实现一定程度上的剪枝降低复杂度
  - 使用该操作时数据量一定要足够大

更改时，只需要修改函数：
```Python
def Generate():
    # 创建一个空的 DataFrame,后续可以修改以生成所有组合  
    df = pd.DataFrame()  
    return df
```

</font>