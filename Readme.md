项目背景：信息流广告投放的核心是创意，什么样的元素什么样的风格是被平台流量所接受的，都在数据里。借助自建的标签体系，将创意“白盒化”开展分析，指导后续的创意制作思路。
1. 项目负责：@余哲闻 
2. 项目估时：4周
3. 工作内容
  1. 广告视频素材手工打标签
  2. 标签数据清洗与统计描述
  3. 分类算法、因子打分
4. 工作目标：输出因子评分/重要性得分/SHAP分析报告（参考模型输出），对创意数据进行归因分析
5. 证券类广告投放情况概览：飞书链接：信息流广告竞品分析 副本    密码：MZyp
6. 过往showcase
  1. 历史打标情况：信息流广告视频标签 
  2. 历史模型使用：PCA、LR、评分卡模型、lightGBM/XGBoost或自己常用/想用的
  3. 语言：不限
  
---

一 样本打标
筛选规则：2023年上线 ∩ 消费≥1k
分析样本202308.xlsx
2023素材标签分析 
二 统计描述
产能和创意的集中性分布
2.1 数据透视研究分析
2.1.1 视频形式、数量随时间变化分析
暂时无法在飞书文档外展示此内容
根据统计透视数据视图，可以看到：
- 视频形式大都以口播为主，并且在2023年4月、5月中形式更为多样化（体现在除口播外其他形式的占比相对较高）。6月之后口播明显占据大比例，同时7月其他形式虽然数量有所提升，但由于7月视频数量相对较多导致比例明显较低。同时8月样本量不足暂不作讨论。
- 通过不同时间广告视频数量的统计可以发现：2023年4月，2023年5月，2023年6月产出广告视频较为均衡，大约15条左右；2023年7月则明显增多，可能由于暑假和经济形势影响，为其他时间两倍。2023年8月由于样本量不足暂不作讨论。
2.1.2 视频前贴随时间变化分析
暂时无法在飞书文档外展示此内容
根据统计透视数据视图，可以看到：
- 5月份视频前贴数量最多，随后趋于平缓。
- 前贴形式以无特效普通文字居多。
2.1.3  片尾营销链路随时间变化分析
暂时无法在飞书文档外展示此内容
根据统计透视数据视图，可以看到：
- 除样本量过少的8月外，4月，5月，6月，7月片尾链路为开户的都大约为12条左右，十分稳定。也可以看出开户作为片尾营销链路的重要性。
- 其中，从6月开始零星出现企微的营销方式，7月则井喷式爆发,7月发布的广告基本是前几个月月均的两倍，主要就体现在企微相关广告基本占据了半壁江山，与开户广告数量基本持平。
- 片尾营销链路为通用的视频每个月都有，且数量上略有增长，比例上基本不变。
- 其中，营销方式以开户有优惠、服务好为主，也有不少视频强调线上开户方便。其他营销方式如紧迫感，心灵鸡汤等虽有但数量很少。
2.1.4 人物形象、场景与展示数分析
暂时无法在飞书文档外展示此内容
根据统计透视数据视图，可以看到：
- 总的来说，休闲装和正装人物展示数相近，正装较多，可能是因为正装视频本身比例较高。
- 从场景来说，正装展示数高的场景多为办公楼下、老板办公室、茶室、休息区，为主要体现成功人士生活的场景；休闲装展示书高的场景则多为办公楼下、街景、走廊，主要体现贴近生活的场景。
三 数据建模
3.1 打标材料数值化
3.1.1 为什么要数值化
将一张Excel数值化之后再进行数学建模和数据挖掘有以下好处：
1. 方便数据处理和分析： 数学建模和数据挖掘通常需要对数据进行各种数学运算和统计分析。通过将Excel中的数据数值化，可以更方便地进行这些操作，如计算统计指标、应用数学模型、进行机器学习等。
2. 提高计算效率： Excel中的数据通常以电子表格的形式存储，包含大量的单元格和格式化信息。将数据数值化后，可以将其转换为更高效的数据结构，如数组、矩阵或数据帧，以便更快地进行计算和分析。
3. 消除数据不一致性： Excel文件中可能存在各种格式、公式、空白单元格等，这可能导致数据的不一致性和处理上的困难。通过数字化数据，可以消除这些问题，确保数据的一致性和可靠性。
4. 支持更多的分析方法： 数学建模和数据挖掘方法通常基于数值数据进行分析。通过将Excel数据数值化，可以利用更广泛的数学建模和数据挖掘技术，如回归分析、聚类分析、分类算法、神经网络等，以发现数据中的模式、关联和趋势。
5. 与其他工具的兼容性： 数学建模和数据挖掘通常使用各种编程语言和工具进行实现，如Python、R、MATLAB等。将Excel数据数值化后，可以更容易地将其导入到这些工具中进行进一步的分析和建模。
本次分析，我们主要使用Python和其他工具进行数据建模和数据挖掘。
3.1.2 通过独热编码对打标数据进行数值化
由于打标后的表格满足以下条件：其中每一列都是一个下拉表单，而每行数据都是每一列表单的一个选项。所以我们考虑使用独热编码（One-Hot Encoding）来将这些分类数据数值化。
独热编码是一种常用的特征编码方法，它将每个分类变量转换为一个二进制向量，其中只有一个元素为1，表示当前的选项，而其他元素都为0。这种编码方式可以将分类变量转换为数值变量，以便在机器学习模型中使用。
为了将数据转化成独热编码，我们编写了一个简单的脚本。
import pandas as pd  
  
# 读取Excel文件  
data = pd.read_excel('matrix_ex01.xlsx')  
  
# 对每一列进行独热编码  
encoded_data = pd.get_dummies(data, columns=data.columns[6:])  
  
# 将清洗后的数据输出为CSV文件  
encoded_data.to_csv('one_hot.csv', index=False)  
3.1.3 数据清理与离群值筛选
离群值（Outliers）是指在数据集中与其他观测值显著不同的异常值。它们可能是由于测量错误、数据录入错误、异常事件或统计误差等原因引起的。离群值的存在可能会对数据分析和建模产生负面影响，因此在进行数据分析前，通常需要对离群值进行筛选和清洗。
筛选离群值的方法可以有多种，常见的方法包括基于统计指标的方法和基于距离的方法：
1. 基于统计指标的方法：这种方法使用统计学原理来识别离群值。常见的统计指标包括均值、标准差、四分位数等。通过计算数据点与这些统计指标之间的偏差或距离，可以确定是否为离群值。例如，常用的方法是使用均值加减几倍标准差的范围来定义离群值的阈值。
2. 基于距离的方法：这种方法基于数据点之间的距离或相似度来判断离群值。常见的方法包括K近邻算法、聚类分析等。通过计算数据点与其他点之间的距离或相似度，可以识别出与其他点差异较大的离群值。
数据清洗的目的是识别和处理离群值，以提高数据的质量和准确性。数据清洗的原理和作用如下：
1. 原理：数据清洗通过识别和处理离群值来修复或剔除异常数据，以确保数据的可靠性和一致性。清洗过程通常包括以下步骤：
  - 离群值检测：使用适当的方法和阈值来识别离群值。
  - 离群值处理：根据具体情况，可以采取不同的处理方式，如替换为缺失值、平均值或中位数，或者直接剔除离群值所在的数据点。
  - 数据修复：对于存在错误或缺失值的数据，可以尝试使用插值、回归等方法进行修复，以恢复数据的完整性和准确性。
2. 作用：数据清洗对于数据分析和建模具有重要的作用：
  - 提高数据质量：清洗离群值可以减少异常数据对整体数据质量的影响，提高数据的准确性和可靠性。
  - 改善模型效果：离群值可能对数据模型产生不良影响，通过清洗离群值可以改善模型的拟合效果和预测准确性。
  - 保护分析结果的可信度：在进行统计分析和数据挖掘时，清洗离群值可以提高结果的可信度，避免误导性的分析结论。
  - 数据预处理：数据清洗是数据预处理的一部分，为后续的数据分析、建模和可视化提供干净、一致的数据集。
综上所述，离群值是与其他观测值显著不同的异常值，数据清洗通过识别和处理离群值来提高数据质量、改善模型效果和保护分析结果的可信度。清洗过程包括离群值检测和处理，可以基于统计指标或距离等方法进行。清洗后的数据可以更好地支持数据分析、建模和可视化等任务。
在这里我们给出了一个脚本Outlier.py，它内部有已经实现的离群值筛选函数，图表生成函数以及尚未实现的数据清洗函数的接口。由于样本量不大，我们这次不考虑数据清洗。但是随着样本量上升，后续工作中完全可以添加关于数据清洗的函数。并随需求选择自己的清洗方式。
3.2 线性回归（Linear Regression）分析
3.2.1 线性回归概述
线性回归（Linear Regression）是一种用于建立线性关系的回归算法，用于预测一个连续的数值输出。它通过拟合一条直线（在一维情况下）或一个超平面（在多维情况下）来描述自变量与因变量之间的线性关系。线性回归的目标是找到最佳拟合线（或超平面），使得预测值与实际值之间的误差最小。
3.2.2 分析过程
通过编写Python脚本，我们对转化为独热编码的csv文件进行线性回归分析，并根据脚本生成线性回归的图片，打印斜率最大的自变量名称。
脚本返回的图片如下所示：
[图片]
同时，脚本还打印了对于每一个因变量，当前分析变量的斜率绝对值最大的两条线的自变量名称。
Dependent Variable: 平均千次展现费用(元)
Variables with Maximum Slope: 前贴形式_红底花字蹦跳元素, 道具_书本

Dependent Variable: 点击率(%)
Variables with Maximum Slope: 场景_办公楼下, 片尾引导方式_紧迫感话术

Dependent Variable: 3秒播放率
Variables with Maximum Slope: 前贴形式_股市相关（K线、金钱）, 视频形式_剧情+混剪

Dependent Variable: 25%进度播放率
Variables with Maximum Slope: 前贴形式_股市相关（K线、金钱）, 视频形式_剧情+混剪
3.2.3 线性分析的简单结论
由此可见，股市相关的前贴和剧情+混剪的视频形式有效提高了播放率，办公楼下的场景和紧迫感的话术则有效提高了点击率。但是，由于样本量太小，很可能是偶然导致数值偏离。
综上所述，根据线性回归分析，我们可以选择前贴+剧情+混剪，场景可以选择办公楼下，营销话术则可以选择紧迫感。当然，由于二值分析的局限性，线性回归的结论可能仅作参考。
3.3 XGBoost分析
3.3.1 XGBoost概述
XGBoost（eXtreme Gradient Boosting）是一种基于梯度提升树的机器学习算法，可用于回归和分类问题。XGBoost回归是指使用XGBoost算法进行回归任务，即通过构建多个弱学习器的集合来拟合数据，并预测连续型目标变量的值。
而对于我们正在进行的小数据量的数据挖掘任务，选择XGBoost回归有以下好处：
1. 高准确性：XGBoost在处理小数据集时表现出色，它通过集成多个弱学习器来提高预测的准确性。XGBoost使用了一种特殊的优化技术，能够减少过拟合的风险，同时在训练集和测试集上都能获得较高的预测准确性。
2. 快速训练速度：XGBoost在处理小数据集时通常具有较快的训练速度。它通过并行计算和有效的数据结构优化，能够高效地处理大量特征和样本。这对于小数据集来说，可以更快地完成模型训练和调参的过程。
3. 对缺失值和异常值的鲁棒性：XGBoost能够处理缺失值和异常值，不需要对其进行额外的处理。它可以自动处理缺失值，并且在构建树的过程中对异常值具有一定的鲁棒性，不容易受到异常值的影响。
4. 特征重要性评估：XGBoost提供了对特征重要性的评估，可以帮助你了解哪些特征对于模型的预测性能最重要。这对于小数据集的特征选择和特征工程非常有帮助，可以帮助你更好地理解数据并选择最相关的特征。
总之，XGBoost回归在小数据量数据挖掘任务中具有高准确性、快速训练速度、对缺失值和异常值的鲁棒性以及特征重要性评估等优势。这些特点使得XGBoost成为处理小数据集的强大工具，并广泛应用于各种预测和回归问题。
3.3.2 数据预处理
由于数据中的消耗、点击率、播放率等变量受时间变化影响。我们考虑根据时间对它们进行降权处理。
3.3.2.1 消耗数据预处理
在广告发布的数据挖掘实验中，考虑到消耗作为一个绝对值变量的特性以及时间的影响，我们需要进一步探讨如何采取合适的降权策略。我们选择使用线性降权法，其背后的理念是将上线日期较早的广告的权重逐渐降低，以减少其对整体结果的过度影响。
线性降权法的使用有以下几个理由：
1. 公平性和客观性： 通过引入线性降权法，我们可以在数据分析中保持公平性和客观性。较早上线的广告可能由于更长的展示时间而累积了更多的消耗，但这并不意味着其效果一定更好。通过降低较早广告的权重，我们可以减少其对整体结果的偏倚，使得结果更加客观和准确。
2. 避免时间偏差： 上线日期早的广告往往有更多的机会进行展示和点击，从而可能导致其消耗量相对较高。然而，这种差异可能主要是由于时间因素造成的，而非广告本身的效果。通过线性降权法，我们可以消除时间偏差，更准确地评估广告的实际效果。
3. 平衡长期与短期效果： 广告发布的时间点与消耗之间存在一定的关联性。较早上线的广告可能在短期内获得更多的消耗，而较晚上线的广告可能在长期内产生更稳定的效果。使用线性降权法可以平衡长期和短期效果，使得实验结果更具综合性和可靠性。
基于以上理由，使用线性降权法可以在广告发布的数据挖掘实验中更好地考虑时间的影响，确保结果的公正性和准确性。通过合理调整权重，我们能够获得更可靠的实验结果，进而支持决策制定和优化广告策略的过程。
3.3.2.2 点击率、3秒播放率和25%播放率数据预处理
根据广告逻辑，随着投放时间的增长，广告的曝光量逐渐饱和，导致点击率逐渐降低。由于点击率和播放率是以比例形式呈现的指标，而非累计值，因此使用指数降权方法可以更好地反映这种趋势，并提供更准确的分析结果。
以下是支持使用指数降权的论据：
1. 饱和效应： 广告投放时间的增加会导致广告的曝光量逐渐饱和，即潜在受众已经接触过广告的概率逐渐增大。在饱和状态下，进一步的曝光对于点击率和播放率的提升效果将逐渐减弱。这种非线性的饱和效应支持使用指数降权方法，以更好地反映点击率和播放率随时间变化的趋势。
2. 用户疲劳： 长时间的广告投放可能导致潜在受众对广告感到疲劳，降低了他们对广告的兴趣和点击的意愿。随着时间的推移，用户可能对广告内容产生麻木感，从而导致点击率和播放率逐渐下降。指数降权方法能够更好地捕捉到这种用户疲劳效应，确保在数据挖掘实验中对其进行合理的考虑。
3. 长期效果的重要性： 广告的长期效果对于评估广告投放的成功与否至关重要。较长时间的广告投放可以提供更多的数据和趋势信息，以更全面地评估广告的效果。指数降权方法能够平衡短期和长期效果，更准确地反映广告投放的整体表现。
综上所述，使用指数降权方法在广告投放数据挖掘中具有一定的合理性和支持性。考虑到点击率和播放率的非线性变化趋势，指数降权能够更好地反映广告投放时间对这些指标的影响，提供更准确和可靠的分析结果，从而为广告决策和优化提供有力支持。
3.3.3 XGBoost分析过程
通过编写Python脚本，我们实现了训练XGBoost模型并输出预测的视频配置以及打分表格。
首先，我们使用一个循环，对每个因变量进行分析。在每次循环中，首先将自变量和因变量划分为训练集和测试集。然后，将训练集的自变量和因变量转换为xgb.DMatrix对象，用于训练XGBoost模型。接下来，设置模型的参数，包括目标函数为均方误差（reg:squarederror），学习率为0.1，最大树深度为3。然后，使用训练集数据训练XGBoost模型，并使用测试集数据进行预测。预测结果中找到使因变量最大和最小的一组自变量，并将其保存到一个DataFrame对象result_df中。最后，将result_df保存为Excel文件，文件名为"{target_col}_analysis.xlsx"，并存放在之前创建的结果目录中。
for target_col in target_columns:  
    # 创建目标变量和特征变量的数据集  
    X = data[feature_columns]  
    y = data[target_col]  
    train_size = len(data)  # 使用所有数据作为训练集  
    X_train = X[:train_size]  # 将所有数据用于训练和测试
    y_train = y[:train_size]  # 将所有数据用于训练和测试
    X_test = Generate()  # 函数接口，尚未实现
    train_dates = pd.to_datetime(data['创建时间'][:train_size])  
    train_weights = (today - train_dates).dt.days.apply(lambda x: 0.9 ** x)  
    dtrain = xgb.DMatrix(X_train, label=y_train)  
    dtest = xgb.DMatrix(X_test)  
    params = {  
        'objective': 'reg:squarederror',  
        'eta': 0.1,  
        'max_depth': 3  
    }  
    model = xgb.train(params, dtrain)  
    predictions = model.predict(dtrain)  
    max_idx = predictions.argmax()  
    min_idx = predictions.argmin()  
    max_row = X_train.iloc[max_idx]  # 后续如果实现的话可以改成X_test
    min_row = X_train.iloc[min_idx]  # 后续如果实现的话可以改成X_test  
    result_df = pd.DataFrame({'Max': max_row, 'Min': min_row}).T  
    result_file = os.path.join(result_dir, f"{target_col}_analysis.xlsx")  
    result_df.to_excel(result_file, index=False)  
    importance = model.get_score(importance_type='gain')  
    feature_scores = pd.DataFrame(list(importance.items()), columns=['Feature', 'Score'])  
    feature_scores = feature_scores.sort_values(by='Score', ascending=False)  
    plt.figure(figsize=(15, 10))  
    plt.barh(feature_scores['Feature'], feature_scores['Score'])  
    plt.xlabel('Feature Score')  
    plt.ylabel('Features')  
    plt.title(f'Feature Importance - {target_col}')  
    plt.tight_layout()    
    result_file = os.path.join(result_dir, f"{target_col}_feature_importance.png")  
    plt.savefig(result_file)  
    plt.close()     
    feature_scores_file = os.path.join(result_dir, f"{target_col}_feature_scores.xlsx")    
    feature_scores.to_excel(feature_scores_file, index=False
然后，我们计算特征重要性分数，并绘制柱状图。首先，使用model.get_score(importance_type='gain')获取特征重要性分数，其中importance_type='gain'表示使用增益作为评估指标。然后，将部分特征重要性分数保存到一个DataFrame对象feature_scores中，并按分数降序排序。接下来，创建一个图形窗口，设置图形的大小为15x10，绘制水平柱状图，横轴为特征名，纵轴为特征重要性分数。设置横轴标签为"Feature Score"，纵轴标签为"Features"，标题为"Feature Importance - {target_col}“，并自动调整图形布局。最后，将柱状图保存为PNG图片，文件名为”{target_col}_feature_importance.png"，并存放在结果目录中。通过输出打分图像来对结果进行可视化。
3.3.4 XGBoost分析结论
3.3.4.1 最佳策略输出表
我们通过脚本输出了对于五个监督变量分别的最优策略Excel表，第一行为Max，第二行为Min：
- 消耗
暂时无法在飞书文档外展示此内容
- 平均前次展现费用（元）
暂时无法在飞书文档外展示此内容
- 点击率
暂时无法在飞书文档外展示此内容
- 3秒播放率
暂时无法在飞书文档外展示此内容
- 25%进度播放率
暂时无法在飞书文档外展示此内容
3.3.4.2 可视化图表
通过筛选几项最有意义的打分策略并可视化，我们可以适当对tag进行打分。
消耗
暂时无法在飞书文档外展示此内容
[图片]
平均千次展现费用（元）
暂时无法在飞书文档外展示此内容
[图片]
点击率
暂时无法在飞书文档外展示此内容
[图片]
3秒播放率
暂时无法在飞书文档外展示此内容
[图片]
25%进度播放率
暂时无法在飞书文档外展示此内容
[图片]
3.3.5 参考结论
消耗
根据得分表格，可以得出以下分析和解释：
1. 片尾链路_企微的得分最高，为 450,957,536。这意味着在广告消耗的预测中，片尾链路_企微是最重要的特征。片尾链路_企微可能代表了广告投放后在微信企业号中的链接或相关内容。较高的得分表明该特征与广告资金消耗之间存在着较强的关联，可能是因为通过企微渠道进行投放能够带来较高的曝光量、点击率或转化率，从而使得广告消耗增加。
2. 前3s演员演绎方式_唱歌的得分为 341,760,096，居于第二位。这表明演员唱歌方式在广告资金消耗的预测中具有较大的影响力。这可能意味着通过演员唱歌的方式能够吸引观众的注意力，提高广告的记忆度和吸引力，从而导致更高的广告消耗。
3. 前3s演员演绎方式_正式口播和前3s演员演绎方式_无分别得分为 192,577,376 和 77,512,824，表明演员的正式口播方式对广告资金消耗的预测有一定的影响，而无演绎方式的影响相对较小。这可能意味着通过演员的正式口播方式能够增加广告的专业感和说服力，从而对资金消耗产生积极影响。
4. 场景_老板办公室（办公桌、电脑）的得分为 70,807,056，位于前列。这表明广告在老板办公室场景中的呈现对广告资金消耗有一定的预测能力。这可能是因为在老板办公室场景中展示广告能够吸引目标受众的关注，并与高价值的商业环境相关联，从而导致更高的广告资金消耗。
平均千次展现费用（元）
根据这些特征的得分，可以得出以下结论：
1. 视频形式_鬼畜（猫狗、RAP、跳舞等）得分最高，为53,167.57。这意味着在这种视频形式下，平均千次展现费用较高，可能与该类型视频的制作成本、受众受欢迎程度等因素有关。
2. 片尾链路_通用和片尾画面_APP图标得分也相对较高，分别为46,626.54和40,423.70。这表明在片尾添加通用的链接或显示APP图标可能会增加广告费用。
3. 其他得分较高的特征包括道具_书本、bgm类型_明快、视频形式_口播+混剪等。这些特征可能与广告的制作复杂度、音乐选择、视频形式等因素有关，从而影响广告费用。
点击率
根据得分表格，可以得出以下分析和解释：
1. 场景_办公楼下的得分最高，为 153.52。这意味着在点击率的预测中，场景_办公楼下是最重要的特征。这可能表明在广告中展示办公楼下的场景能够吸引观众的注意力，提高点击率。
2. bgm类型_舒缓和片尾画面_开户界面分别得分为 68.22 和 66.41，居于第二和第三位。这表明舒缓类型的背景音乐和展示开户界面的片尾画面对点击率有较大的影响力。舒缓的背景音乐可能能够营造轻松愉悦的氛围，而开户界面的展示可能引起观众的兴趣和好奇心，从而提高点击率。
3. 片尾链路_通用的得分为 33.02，位于前列。这表明通用的片尾链路对点击率有一定的预测能力。通用的片尾链路可能具有较广泛的适用性，能够吸引更多观众的点击行为。
3秒播放率
在数据清理剔除有关片尾的打分之后，我们重新分析这些特征：
1. 场景_居家（客厅、厨房、卧室）的得分最高，为 1,393.70。这意味着在前三秒内展示居家场景可以对播放率产生较大的影响。居家场景可能能够让观众产生共鸣，提高他们的兴趣和参与度。
2. 其他得分较高的特征包括bgm类型_激昂、前贴音效_女声配音、bgm类型_明快等。这些特征在前三秒的播放率预测中具有一定的影响力。
3. 场景_走廊的得分仍然最低，为 95.75。这表明在前三秒内展示走廊场景对播放率的预测能力较弱。在广告中展示走廊场景可能不太能吸引观众的注意力，在前三秒内影响播放率。
25%进度播放率
在数据清理剔除有关片尾的打分之后，我们重新分析这些特征：
1. 特征得分最高的是bgm类型_激昂，得分为2,095.74。这意味着在前三秒内使用激昂的背景音乐类型可能对实现25%的播放率有积极的影响。
2. 其他得分较高的特征包括bgm类型_明快、场景_绿幕、视频形式_口播+混剪等。这些特征在实现25%的播放率中可能具有一定的影响力。
3. 场景_走廊的得分仍然最低，为14.00。这表明在前三秒内展示走廊场景可能对实现25%的播放率的预测能力较弱。
3.4 决策树回归算e法
决策树（Decision Trees） 是一种简单而直观的算法，适用于小型数据集。它可以根据数据的特征进行分割，并生成一个树形结构来进行预测。决策树易于解释和理解，可以用于分类和回归任务。
但是，决策树算法通常用于处理离散的因变量（也称为分类变量）。决策树的主要目标是根据自变量的特征将数据集划分为不同的类别。每个节点都代表一个特征，分支代表该特征的不同取值，叶节点代表最终的类别标签。对于连续的因变量（也称为回归变量），可以使用回归树算法或基于决策树的回归方法，如CART（Classification and Regression Trees）算法。这些算法在决策树的基础上进行了扩展，以处理连续的输出。
3.5 结论、思考与不足
3.5.1 最终参考结论
3.5.2 思考、不足与后续展望
1. 由于样本量太少，我们选择的线性回归与XGBoost模型都有过拟合的情况发生。XGBoost模型也没有充分对每一个稀疏矩阵中的变量进行分析与打分。这导致模型输出的最优解和参考权重分数都置信度不高。当然，由于正确的数据处理和分析过程，再加上XGBoost模型良好的鲁棒性，这些数据也具有可读性和实践上的意义。
2. 在使用XGBoost模型分析时，由于模型过拟合限制，我们输出最优解的备选集合只有输入的数据集，而没有采用遍历所有可能解的情况。当然，我们在脚本中预留了函数接口，可以实现并输出遍历全部可行解的测试数据集。当然，需要注意的是：
  - 生成测试集时多个自变量本身具有限制关系
  - 可以先输出打分表格实现一定程度上的剪枝降低复杂度
  - 使用该操作时数据量一定要足够大
3. 本次数据处理只是简单的转化为独热编码，由于数据只有85条，没有筛选离群值与数据清洗。但是脚本已经完成编写，数据清洗也预留了可选择的接口。在数据量大以后可以选择合适的数据清理方式对数据预处理。
4. 后续可以选择其他模型对数据进行分析处理。
5. 后续可以考虑使用CV（计算机视觉）模型对视频进行打标分析。